{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa5c149",
   "metadata": {},
   "source": [
    "### Text Clustering with kMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c03b57",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "In our example below we are using the 20newsgroup dataset that is available in Scikit-learn datasets. This dataset consists article of 20 groups but in our example, we are filtering only for two categories soc.religion.christian and comp.graphics. Lets load dataset for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a178d8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 787)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['soc.religion.christian',\n",
    "              'comp.graphics']\n",
    "# Load Data\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Check number of records in training and testing data\n",
    "len(twenty_train.data),len(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cecac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: dan@ingres.com (a Rose arose)\\nSubject: Re: earthquake prediction\\nOrganization: Representing my own views here.\\nLines: 75\\n\\nmserv@mozart.cc.iup.edu (Mail Server) writes:\\n: Ok, a few days back, the below-included message was posted stating: \\n: \\n: >     I believe with everything in my heart that on May 3, 1993, the city of\\n: >Portland, Oregon in the country of the United States of America will be hit\\n: >with a catastrophic and disastrous earthquake...\\n: \\n: By now, we know that this did not come to pass....\\n: \\n: ...I don\\'t think it\\'s particularly \\n: glorifying to God to say things like \"Well, I THINK the Lord is telling me...\", \\n: ..Such statements seem to me to be an attempt to get a spiritual thrill should \\n: the guess happen to come true, without risking the guilt of false prophecy \\n: should it fail to come to pass.  I do not believe genuine prophecy was ever \\n: like this.  Comments?\\n: \\n\\nI agree.   People should not be misled to believe \"thus sayeth the Lord\" by\\ninnuendo or opinion or speculation.\\n\\nSpeak directly.  If the Lord has given you something to say, say it.\\nBut, before I declare \"thus sayeth the Lord\", I\\'d better know for certain\\nwithout a shadow of a doubt that I am in the correct spiritual condition\\nand relationship with the Lord to receive such a prophecy and be absolutely\\ncertain, again, without the tiniest shadow of a doubt that there is no\\npossibility of my being misled by my own imaginations or by my hope of gaining\\nrecognition or of being misled by the wiles of the devil and his followers.\\n\\nMistakes in this area are costly and dangerous.  For me, my greatest fears\\nin this area would be the following:\\n\\n1--that the people would be misled\\n2--that people would lose respect for christianity\\n3--that true prophecy would be clouded by all the false prophecies\\n4--were God to call me to be a prophet and I were to misrepresent God\\'s Word,\\n   my calling would be lost forever.  God\\'s Word would command the people\\n   never to listen to or fear my words as I would be a false prophet.  My\\n   bridges would be burnt forever.  Perhaps I could repent and be saved, but\\n   I could never again be a prophet of God.\\n\\nIn the light of this, it is critical that we speak when the Lord says speak\\nand that we be silent when the Lord says to be silent lest we deprive the\\nworld of God\\'s Word and hide it under a bushel either by our inappropriate,\\ncowardly silence or by our false statements.  And because of this, it\\nis critically important that we remain close to the Lord, in His Word, and\\nin prayer, and filled with the Spirit of God so that we know the difference.\\n\\nIn this day and age, sinners spout off their mouths left and right judging\\none another, claiming \"rights\" that are not theirs, denying rights that do\\nindeed belong to others, demanding equal respect for all the \"gods\" of this\\nworld, and uttering every form of falseness that promises to make one feel\\ngood.\\n\\nIt\\'s time that we christians give an example of honesty that stands out in\\ncontrast against this backdrop of falsehood.  When we say, \"thus sayeth the\\nLord\", it happens.  When we pray, prayer is answered because we prayed right.\\nWhen we say we\\'re christians, we really mean it.\\n\\n           Dan\\n\\n--\\n-----------------------------------------------------------------------\\n\\t\"I deplore the horrible crime of child murder...\\n\\t We want prevention, not merely punishment.\\n\\t We must reach the root of the evil...\\n\\t It is practiced by those whose inmost souls revolt\\n\\t from the dreadful deed...\\n\\t No mater what the motive, love of ease,\\n\\t\\tor a desire to save from suffering the unborn innocent,\\n\\t\\tthe woman is awfully guilty who commits the deed...\\n\\t but oh! thrice guilty is he who drove her\\n\\t\\tto the desperation which impelled her to the crime.\"\\n\\n\\t\\t- Susan B. Anthony,\\n\\t\\t  The Revolution July 8, 1869\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346251b8",
   "metadata": {},
   "source": [
    "### Generate the features using TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645fb604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 22690)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Feature Generation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# # Vectorize document using TF-IDF\n",
    "tf_idf_vect = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# Fit and Transfrom Text Data\n",
    "X_train_counts = tf_idf_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "# Check Shape of Count Vector\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a189454",
   "metadata": {},
   "source": [
    "### perform GMM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a13a516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-79a736564d84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_sklearn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdensity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_sklearn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_sklearn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprediction_gmm\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'viridis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import numpy as np\n",
    "\n",
    "sklearn_pca = TruncatedSVD(n_components = 2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_train_counts)\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full').fit(Y_sklearn)\n",
    "prediction_gmm = gmm.predict(Y_sklearn)\n",
    "probs = gmm.predict_proba(Y_sklearn)\n",
    "\n",
    "centers = np.zeros((3,2))\n",
    "for i in range(3):\n",
    "    density = mvn(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(Y_sklearn)\n",
    "    centers[i, :] = Y_sklearn[np.argmax(density)]\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction_gmm ,s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1],c='black', s=300, alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd0be7",
   "metadata": {},
   "source": [
    "### Evaluate Clustering Performance\n",
    "\n",
    "First, we can evaluate the clustering using Davies-Bouldin Index and Silhouette Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a577dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Compute DBI score\n",
    "dbi = metrics.davies_bouldin_score(X_train_counts.toarray(), pred_labels)\n",
    "\n",
    "# Compute Silhoutte Score\n",
    "ss = metrics.silhouette_score(X_train_counts.toarray(), pred_labels , metric='euclidean')\n",
    "\n",
    "# Print the DBI and Silhoutte Scores\n",
    "print(\"DBI Score: \", dbi, \"\\nSilhoutte Score: \", ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704989cd",
   "metadata": {},
   "source": [
    "Now Evaluate Clustering Performance using WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5958fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordCloud and STOPWORDS\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def word_cloud(text,wc_title,wc_file_name='wordcloud.jpeg'):\n",
    "    # Create stopword list\n",
    "    stopword_list = set(STOPWORDS) \n",
    "\n",
    "    # Create WordCloud \n",
    "    word_cloud = WordCloud(width = 800, height = 500, \n",
    "                           background_color ='white', \n",
    "                           stopwords = stopword_list, \n",
    "                           min_font_size = 14).generate(text) \n",
    "\n",
    "    # Set wordcloud figure size\n",
    "    plt.figure(figsize = (8, 6)) \n",
    "    \n",
    "    # Set title for word cloud\n",
    "    plt.title(wc_title)\n",
    "    \n",
    "    # Show image\n",
    "    plt.imshow(word_cloud) \n",
    "\n",
    "    # Remove Axis\n",
    "    plt.axis(\"off\")  \n",
    "\n",
    "    # save word cloud\n",
    "    plt.savefig(wc_file_name,bbox_inches='tight')\n",
    "\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({\"text\":twenty_train.data,\"labels\":pred_labels})\n",
    "\n",
    "\n",
    "for i in df.labels.unique():\n",
    "    new_df=df[df.labels==i]\n",
    "    text=\"\".join(new_df.text.tolist())\n",
    "    word_cloud(text,twenty_train.target_names[i], twenty_train.target_names[i]+'.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da1dd9",
   "metadata": {},
   "source": [
    "Reference: https://machinelearninggeek.com/text-clustering-clustering-news-articles/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
