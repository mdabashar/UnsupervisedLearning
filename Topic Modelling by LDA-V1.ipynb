{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regular expression help: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "from pprint import pprint\n",
    "\n",
    "# Data cleaning\n",
    "import re # RegEx for regular expression\n",
    "\n",
    "# Preprocess\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\") # Choose a language\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Topic modelling LDA\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Visualisation\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# custom\n",
    "from mab_text_utils import MabTextUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\RRR_Data\\\\CorporaToAnalysis\\\\'\n",
    "fins = ['Flower1907\\\\Flower1907.txt', 'BarnardDavis\\\\BarnardDavis.txt']\n",
    "track=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''\n",
    "with open(BASE+fins[track], mode='r', encoding='utf8') as FI:\n",
    "    full_text = FI.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearn_text_for_lda(full_text):\n",
    "    result_text = ''.join(i for i in full_text if ord(i) < 128) # remove non english characters\n",
    "    result_text = re.sub(r'\\d+', '', result_text) # remove numbers\n",
    "    result_text = result_text.replace('”', '').replace('“', '').replace('‘', '').replace('’', '') # remove beauty quoute\n",
    "    result_text = re.sub(r' +', ' ', result_text) # remove extra space\n",
    "    result_text = re.sub(r'(\\n )+', '\\n', result_text) # remove space after newline\n",
    "    result_text = re.sub(r'(\\n){2,}', '<p>', result_text) # remove multiple newlines and mark by <p>\n",
    "    result_text = re.sub(r'(<p>+\\W?\\w? <p>*)+', '<p>', result_text) #remove interlanced multiple newliens\n",
    "    result_text = re.sub(r'\\n', ' ', result_text) # replace single newline with space as these newlines are page wrapings\n",
    "    result_text = re.sub(r'[^\\w\\s<>]', '', result_text) # remove all punctuations\n",
    "    result_text = re.sub(r'( <p>)+', '<p>', result_text) # remove reitative <p>\n",
    "    result_text = re.sub(r' +', ' ', result_text) # remove repitative spaces\n",
    "    result_text = result_text.replace('<p>', '\\n') # replace <p> with newline as <p> marks new paragraph\n",
    "    result_text = re.sub(r'[<>]', '', result_text) # remove all punctuations\n",
    "    result_text = result_text.lower() # convert to lower case\n",
    "    return [line.strip() for line in result_text.split('\\n')] # split into paragraphs and remove extra spaces\n",
    "\n",
    "result_text = clearn_text_for_lda(full_text)\n",
    "#print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a paragraph to preview after preprocessing.\n",
    "\n",
    "para_sample = result_text[40]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in para_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess all the paragraphs\n",
    "map_result_text = list(map(preprocess, result_text))\n",
    "pp_paragraphs = []\n",
    "pp_parag2ori_para = []\n",
    "for idx, para in enumerate(map_result_text):\n",
    "    if len(para)>0:\n",
    "        pp_paragraphs.append(para)\n",
    "        pp_parag2ori_para.append(idx)\n",
    "        \n",
    "pp_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the paragraphs into Bag of Words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from \"pp_paragraphs\" containing the number of times a word appears in the training set.\n",
    "dictionary = gensim.corpora.Dictionary(pp_paragraphs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can filter out tokens that appear in\n",
    "    1. less than 15 documents (absolute number) or\n",
    "    2. more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "    3. after the above two steps, keep only the first 100,000 most frequent tokens.\n",
    "'''\n",
    "# dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(para) for para in pp_paragraphs]\n",
    "bow_corpus[3410]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document.\n",
    "\n",
    "bow_doc_4310 = bow_corpus[3410]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the paragraphs into TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "   \n",
    "pprint(corpus_tfidf[3410])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise topics using pyLDAvis\n",
    "\n",
    "#### Note in the visualisation\n",
    "\n",
    "Saliency: a measure of how much the term tells you about the topic.\n",
    "\n",
    "Relevance: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n",
    "\n",
    "The size of the bubble measures the importance of the topics, relative to the data.\n",
    "\n",
    "First, we got the most salient terms, means terms mostly tell us about what’s going on relative to the topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us look at topic distribution to paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topic_rel(para_idx):\n",
    "    topic_idx, rel = sorted(lda_model_tfidf[bow_corpus[para_idx]], key=lambda tup: -1*tup[1])[0]\n",
    "    return topic_idx, rel\n",
    "\n",
    "para_id = 4310\n",
    "get_top_topic_rel(para_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "para2topic = []\n",
    "for para_idx in range(len(pp_paragraphs)):\n",
    "    topic_idx, rel = get_top_topic_rel(para_idx)\n",
    "    para2topic.append(topic_idx)\n",
    "\n",
    "for idx, p2t in enumerate(para2topic):\n",
    "    if p2t==7:\n",
    "        print(result_text[pp_parag2ori_para[idx]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
