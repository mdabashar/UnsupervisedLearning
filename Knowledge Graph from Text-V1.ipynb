{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # RegEx for regular expression\n",
    "import spacy # for NLP\n",
    "import neuralcoref # used in pipeline for better NER\n",
    "import pandas as pd # for dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\RRR_Data\\\\CorporaToAnalysis\\\\'\n",
    "fins = ['Flower1907\\\\Flower1907.txt', 'BarnardDavis\\\\BarnardDavis.txt']\n",
    "track=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''\n",
    "with open(BASE+fins[track], mode='r', encoding='utf8') as FI:\n",
    "    full_text = FI.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearn_text_for_lda(full_text):\n",
    "    result_text = ''.join(i for i in full_text if ord(i) < 128) # remove non english characters\n",
    "    #result_text = re.sub(r'\\d+', '', result_text) # remove numbers\n",
    "    result_text = result_text.replace('”', '').replace('“', '').replace('‘', '').replace('’', '') # remove beauty quoute\n",
    "    result_text = re.sub(r' +', ' ', result_text) # remove extra space\n",
    "    result_text = re.sub(r'(\\n )+', '\\n', result_text) # remove space after newline\n",
    "    result_text = re.sub(r'(\\n)+', '\\n', result_text) # remove multiple newlines\n",
    "    result_text = re.sub(r'(\\n)+', ' ', result_text) # remove multiple newlines\n",
    "    #result_text = result_text.split('.')\n",
    "    return result_text\n",
    "\n",
    "result_text = clearn_text_for_lda(full_text)\n",
    "#print(result_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Entity Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "#neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_pairs(text, coref=False):\n",
    "    # preprocess text\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "\n",
    "    def refine_ent(ent, sent):\n",
    "        unwanted_tokens = (\n",
    "            'PRON',  # pronouns\n",
    "            'PART',  # particle\n",
    "            'DET',  # determiner\n",
    "            'SCONJ',  # subordinating conjunction\n",
    "            'PUNCT',  # punctuation\n",
    "            'SYM',  # symbol\n",
    "            'X',  # other\n",
    "        )\n",
    "        ent_type = ent.ent_type_  # get entity type\n",
    "        if ent_type == '':\n",
    "            ent_type = 'NOUN_CHUNK'\n",
    "            ent = ' '.join(str(t.text) for t in\n",
    "                           nlp(str(ent)) if t.pos_\n",
    "                           not in unwanted_tokens and t.is_stop == False)\n",
    "        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "            refined = ''\n",
    "            for i in range(len(sent) - ent.i):\n",
    "                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                    refined += ' ' + str(ent.nbor(i))\n",
    "                else:\n",
    "                    ent = refined.strip()\n",
    "                    break\n",
    "\n",
    "        return ent, ent_type\n",
    "\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = []\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n",
    "                                            'dep': span.root.dep}) for span in spans]\n",
    "        deps = [token.dep_ for token in sent]\n",
    "\n",
    "        # limit our example to simple sentences with one subject and object\n",
    "        if (deps.count('obj') + deps.count('dobj')) != 1\\\n",
    "                or (deps.count('subj') + deps.count('nsubj')) != 1:\n",
    "            continue\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n",
    "                continue\n",
    "            subject = [w for w in token.head.lefts if w.dep_\n",
    "                       in ('subj', 'nsubj')]  # identify subject nodes\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                # identify relationship by root dependency\n",
    "                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n",
    "                if relation:\n",
    "                    relation = relation[0]\n",
    "                    # add adposition or particle to relationship\n",
    "                    if relation.nbor(1).pos_ in ('ADP', 'PART'):\n",
    "                        relation = ' '.join((str(relation), str(relation.nbor(1))))\n",
    "                else:\n",
    "                    relation = 'unknown'\n",
    "\n",
    "                subject, subject_type = refine_ent(subject, sent)\n",
    "                token, object_type = refine_ent(token, sent)\n",
    "\n",
    "                ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                  str(subject_type), str(object_type)])\n",
    "\n",
    "    ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(ent) == '' for ent in sublist)]\n",
    "    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n",
    "                                             'subject_type', 'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(ent_pairs)))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = get_entity_pairs(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_kg(pairs):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    node_deg = nx.degree(k_graph)\n",
    "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
    "    plt.figure(num=None, figsize=(120, 90), dpi=80)\n",
    "    nx.draw_networkx(\n",
    "        k_graph,\n",
    "        node_size=[int(deg[1]) * 500 for deg in node_deg],\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white',\n",
    "        )\n",
    "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
    "                  pairs['relation'].tolist()))\n",
    "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_kg(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Through the Knowledge Graph for Detailed Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graph(pairs, node):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    edges = nx.dfs_successors(k_graph, node)\n",
    "    nodes = []\n",
    "    for k, v in edges.items():\n",
    "        nodes.extend([k])\n",
    "        nodes.extend(v)\n",
    "    subgraph = k_graph.subgraph(nodes)\n",
    "    layout = (nx.random_layout(k_graph))\n",
    "    nx.draw_networkx(\n",
    "        subgraph,\n",
    "        node_size=1000,\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white'\n",
    "        )\n",
    "    labels = dict(zip((list(zip(pairs.subject, pairs.object))),\n",
    "                    pairs['relation'].tolist()))\n",
    "    edges= tuple(subgraph.out_edges(data=False))\n",
    "    sublabels ={k: labels[k] for k in edges}\n",
    "    nx.draw_networkx_edge_labels(subgraph, pos=layout, edge_labels=sublabels,\n",
    "                                font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_graph(pairs, 'skull')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
